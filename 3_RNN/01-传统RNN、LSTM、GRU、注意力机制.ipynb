{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入若干工具包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化rnn对象\n",
    "#第一个参数：input_size (输入张量x的维度)\n",
    "#第二个参数：hidden_size(隐藏层的维度，隐藏层神经元数量)\n",
    "#第三个参数：num_layers(隐藏层的层数)\n",
    "rnn = nn.RNN(5,6,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定输入张量x\n",
    "#第一个参数：sequence_length(输入序列的长度)\n",
    "#第二个参数：batch_size(批次的样本数)\n",
    "#第三个参数：input_size(输入张量x的维度)\n",
    "input1 = torch.randn(1,3,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定初始化的h0\n",
    "#第一个参数：num_layers * num_directions (层数 * 网络方向数)\n",
    "#第二个参数：batch_size(批次的样本数)\n",
    "#第三个参数：hidden_size(隐藏层的维度)\n",
    "h0 = torch.randn(1,3,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2482,  0.6911, -0.2292, -0.4030, -0.3212,  0.4239],\n",
      "         [ 0.1211,  0.3824, -0.8212, -0.6163,  0.6492, -0.8327],\n",
      "         [ 0.2454,  0.9316, -0.8763, -0.0716,  0.9272,  0.0680]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 3, 6])\n",
      "tensor([[[ 0.2482,  0.6911, -0.2292, -0.4030, -0.3212,  0.4239],\n",
      "         [ 0.1211,  0.3824, -0.8212, -0.6163,  0.6492, -0.8327],\n",
      "         [ 0.2454,  0.9316, -0.8763, -0.0716,  0.9272,  0.0680]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "#输入张量放入RNN中，得到输出结果\n",
    "output,hn = rnn(input1,h0)\n",
    "print (output)\n",
    "print(output.shape)\n",
    "print(hn)\n",
    "print(hn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化LSTM对象\n",
    "#第一个参数：input_size(输入张量x的维度)\n",
    "#第二个参数：hidden_size(隐藏层的维度，隐藏层神经元数量)\n",
    "#第三个参数：num_layers(隐藏层的层数)\n",
    "lstm = nn.LSTM(5,6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始换张量x\n",
    "#第一个参数：sequence_length(输入张量序列的长度)\n",
    "#第二个参数：batch_size(批次的样本数量)\n",
    "#第三个参数：input_size(输入张量的维度)\n",
    "input = torch.randn(1,3,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化隐藏层h0、细胞状态层c0\n",
    "#第一个参数：num_layers * num_directions(隐藏层层数 * 方向数)\n",
    "#第二个参数：batch_size(隐藏层批次的样本数)\n",
    "#第三个参数：hidden_size(隐藏层的维度)\n",
    "h0 = torch.randn(2,3,6)\n",
    "c0 = torch.randn(2,3,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0739,  0.1043, -0.2780,  0.1585,  0.1951, -0.0589],\n",
      "         [-0.0552,  0.1211, -0.5487, -0.0940, -0.1131, -0.0312],\n",
      "         [-0.1777, -0.2635, -0.1914, -0.0616, -0.1267, -0.3442]]],\n",
      "       grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "torch.Size([1, 3, 6])\n",
      "tensor([[[ 0.0085,  0.1104,  0.2698,  0.4050, -0.1913,  0.0654],\n",
      "         [ 0.3555,  0.6217, -0.3116, -0.1159,  0.2692,  0.5050],\n",
      "         [-0.4344,  0.2406,  0.0987,  0.0819, -0.2181,  0.2064]],\n",
      "\n",
      "        [[ 0.0739,  0.1043, -0.2780,  0.1585,  0.1951, -0.0589],\n",
      "         [-0.0552,  0.1211, -0.5487, -0.0940, -0.1131, -0.0312],\n",
      "         [-0.1777, -0.2635, -0.1914, -0.0616, -0.1267, -0.3442]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.0140,  0.3142,  1.6726,  0.9672, -0.2457,  0.1374],\n",
      "         [ 0.6373,  1.0368, -0.7615, -0.4118,  0.6075,  1.5129],\n",
      "         [-1.0213,  0.3504,  0.2680,  0.2369, -0.4298,  0.5401]],\n",
      "\n",
      "        [[ 0.1237,  0.1671, -0.6017,  0.2505,  0.5422, -0.0840],\n",
      "         [-0.1677,  0.2345, -0.9841, -0.1371, -0.5350, -0.0924],\n",
      "         [-0.3371, -0.6709, -0.6127, -0.0953, -0.3012, -0.6721]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "#将input、h0、c0放进LSTM中，得到输出\n",
    "output,(hn,cn) = lstm(input,(h0,c0))\n",
    "print(output)\n",
    "print(output.shape)\n",
    "print(hn)\n",
    "print(hn.shape)\n",
    "print(cn)\n",
    "print(cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU(gated recurrent unit)构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化GRU对象\n",
    "#第一个参数：input_size(输入张量的维度)\n",
    "#第二个参数：hidden_size(隐藏层的维度，隐藏层神经元的数量)\n",
    "#第三个参数：num_layers(隐藏层的层数)\n",
    "gru = nn.GRU(5,6,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化输入张量input\n",
    "#第一个参数：sequence_length(输入张量的长度)\n",
    "#第二个参数：batch_size(批次的样本数)\n",
    "#第三个参数：input_size(输入张量的维度)\n",
    "input = torch.randn(1,3,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化隐藏层h0\n",
    "#第一个参数：num_layers * num_directions(隐藏层层数 * 方向数)\n",
    "#第二个参数：batch_size(批次样本数)\n",
    "#第三个参数：hidden_size(隐藏层的维度，隐藏层神经元的个数)\n",
    "h0 = torch.randn(2,3,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3540,  0.6536, -0.3639,  0.0323,  0.6439, -0.9718],\n",
      "         [ 0.0772,  0.2279, -0.0382,  0.1672,  0.2908, -1.3857],\n",
      "         [ 0.0114, -0.1138, -0.0125,  0.3035,  1.2135, -0.3262]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[-0.8296, -0.9264,  0.1194,  0.1818,  0.4957,  0.6590],\n",
      "         [-0.5379,  0.2042, -0.1039,  0.6650,  0.4667, -0.0351],\n",
      "         [-0.0673,  0.6622,  0.3283,  0.2670, -0.8153, -0.1277]],\n",
      "\n",
      "        [[-0.3540,  0.6536, -0.3639,  0.0323,  0.6439, -0.9718],\n",
      "         [ 0.0772,  0.2279, -0.0382,  0.1672,  0.2908, -1.3857],\n",
      "         [ 0.0114, -0.1138, -0.0125,  0.3035,  1.2135, -0.3262]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 3, 6])\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "#将input,h0放进GRU中,得到张量结果\n",
    "output,hn = gru(input,h0)\n",
    "print(output)\n",
    "print(hn)\n",
    "print(output.shape)\n",
    "print(hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "#bmm运算\n",
    "a = torch.randn(10,3,4)\n",
    "b = torch.randn(10,4,5)\n",
    "c = torch.bmm(a,b)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意力机制代码分析\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,query_size,key_size,value_size1,value_size2,output_size):\n",
    "        \"\"\"\n",
    "        初始化函数的参数有5个,query_size表示Q的最后一维大小;key_size表示K的最后一维大小;value_size1与value_size2、1拼接起来是value_size(value_size=(1,value_size1,value_size2))\n",
    "        value_size1代表value的倒数第二维大小,value_size2代表value的最后一维大小;output表示输出的最后一维大小\n",
    "        \"\"\"\n",
    "        super(Attn,self).__init__()\n",
    "        #将参数传入类中\n",
    "        self.query_size = query_size\n",
    "        self.key_size = key_size\n",
    "        self.value_size1 = value_size1\n",
    "        self.value_size2 = value_size2\n",
    "        self.output_size = output_size\n",
    "\n",
    "        #初始化注意力机制计算第一步的线形层\n",
    "        self.attn = nn.Linear(self.query_size + self.key_size,value_size1)\n",
    "\n",
    "        #初始化注意力机制第三步的线性层\n",
    "        self.attn_combine = nn.Linear(self.query_size +value_size2,output_size )\n",
    "\n",
    "    def forward(self,Q,K,V):\n",
    "        \"\"\"\n",
    "        forword函数的参数有三个：Q,K,V，根据模型训练的常识，输入给Attion机制的张量一般都是三维张量，因此假设Q,K,V也是三维张量\n",
    "        \"\"\"\n",
    "        #按照计算规则计算\n",
    "        #采用第一种方法\n",
    "        #将Q,K进行纵轴拼接，再经过线性层变换，再使用softmax得到结果\n",
    "        attn_weight = F.softmax(self.attn(torch.cat((Q[0],K[0]),1)),dim = 1)\n",
    "\n",
    "        #进行第一种方法的后半部分：将得到的权重矩阵与V做矩阵乘法\n",
    "        #当两者都是三维张量且第一维代表batch时，则做bmm计算\n",
    "        attn_applied = torch.bmm(attn_weight.unsqueeze(0),V)\n",
    "\n",
    "        #进行第二步，通过取[0]来降维，需要将Q与第一步计算结果再进行拼接\n",
    "        output = torch.cat((Q[0],attn_applied[0]),1)\n",
    "\n",
    "        #最后是第三步：使用线性层作用在第二步的结果上做一个线性变化扩展维度，得到输出\n",
    "        #要保证输出也是三维张量，因此使用unsequeeze(0)扩展维度\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        return output,attn_weight\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0032,  0.3092, -0.2505, -0.1692,  0.0201,  0.0073, -0.3630,\n",
      "           0.6448, -0.5844,  0.3861, -0.1462,  0.0610,  0.2310, -0.4824,\n",
      "           0.0357,  0.4505, -0.1351,  0.1047,  0.1009, -0.3295,  0.0391,\n",
      "           0.1833,  0.1724, -0.2150, -0.3280,  0.3091,  0.6042,  0.3016,\n",
      "           0.0033, -0.1250, -0.0111,  0.2098,  0.3798,  0.4182,  0.1008,\n",
      "           0.0225,  0.8049,  0.1302, -0.7070,  0.0318,  0.2500, -0.3100,\n",
      "          -0.1419,  0.4988,  0.0781, -0.4034, -0.1098,  0.5854,  0.0441,\n",
      "           0.4562,  0.3125, -0.4676, -0.3024, -0.2297, -0.0061, -0.2875,\n",
      "           0.0410, -0.2929,  0.4175,  0.4556, -0.2975,  0.2235,  0.1215,\n",
      "           0.1929]]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[0.0156, 0.0344, 0.0281, 0.0623, 0.0225, 0.0723, 0.0109, 0.0226, 0.0079,\n",
      "         0.0486, 0.0149, 0.0187, 0.0187, 0.0112, 0.0395, 0.0303, 0.0322, 0.0599,\n",
      "         0.0481, 0.0336, 0.0265, 0.0064, 0.0222, 0.0634, 0.0426, 0.0288, 0.0329,\n",
      "         0.0496, 0.0331, 0.0259, 0.0118, 0.0245]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "query_size = 32\n",
    "key_size = 32\n",
    "value_size1 = 32\n",
    "value_size2 = 64\n",
    "output_size = 64\n",
    "attn = Attn(query_size,key_size,value_size1,value_size2,output_size)\n",
    "Q = torch.randn(1,1,32)\n",
    "K = torch.randn(1,1,32)\n",
    "V = torch.randn(1,32,64)\n",
    "out = attn(Q,K,V)\n",
    "print(out[0])\n",
    "print(out[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
